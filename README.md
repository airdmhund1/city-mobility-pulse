# ğŸš²ğŸŒ¦ City Mobility Pulse â€“ Real-Time Streaming ETL with Spark, Delta Lake & Kafka

## ğŸ“– Overview
**City Mobility Pulse** is a real-time data pipeline that simulates and processes **bike-sharing** and **weather events**.  

The pipeline ingests data from Kafka (via Redpanda), applies streaming transformations in **Apache Spark**, and stores results in a **Delta Lake** using the **Bronze â†’ Silver â†’ Gold architecture**.

This project is useful for:
- Learning **modern streaming ETL pipelines**
- Understanding how **weather impacts bike-sharing usage**
- Building a foundation for **real-time analytics dashboards**

---

## ğŸ— Architecture

```mermaid
flowchart LR
    subgraph Kafka/Redpanda
        A[Producer - Bikes] --> |JSON| B[(Kafka Topic: bike)]
        A2[Producer - Weather] --> |JSON| C[(Kafka Topic: weather)]
    end

    subgraph Spark Structured Streaming
        B --> D[Bronze Delta Table]
        C --> D
        D --> E[Silver Delta Table]
        E --> F[Gold Delta Tables]
    end

    F --> G[(S3 / MinIO / Local FS)]
```

- **Producers** generate fake bike & weather events and push to Kafka/Redpanda.  
- **Spark** reads from Kafka, processes events, and writes into Delta Lake tables.  
- **Delta Lake** stores results in **Bronze, Silver, and Gold layers**.  

---

## âš™ï¸ Tech Stack

- **Apache Spark (Structured Streaming)** â€“ real-time ETL  
- **Delta Lake** â€“ ACID-compliant data storage  
- **Kafka / Redpanda** â€“ event streaming backbone  
- **Python 3.11** â€“ main driver language  
- **Docker & Docker Compose** â€“ local containerized setup  
- **Makefile** â€“ helper commands for local dev  
- **Linters** â€“ black, flake8, pylint, isort for code quality  

---

## ğŸ“‚ Project Layout

```
repo-root/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                  # FastAPI-based service (if running API)
â”‚   â””â”€â”€ streaming/
â”‚       â””â”€â”€ stream_to_delta.py   # Main Spark job
â”‚
â”œâ”€â”€ producers/                # Data producers (bike & weather)
â”‚   â”œâ”€â”€ producer_bike.py
â”‚   â””â”€â”€ producer_weather.py
â”‚
â”œâ”€â”€ common/                   # Shared modules/config
â”‚   â””â”€â”€ conf.py
â”‚
â”œâ”€â”€ requirements.txt          # Python deps
â”œâ”€â”€ pyproject.toml            # Linting/formatting config
â”œâ”€â”€ Dockerfile                # Container build
â”œâ”€â”€ docker-compose.yml        # Local services (Kafka/Redpanda, Spark)
â””â”€â”€ Makefile                  # Helper commands
```

---

## ğŸš€ Getting Started

### 1. Prerequisites
Install these locally:
- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)
- Python 3.11 (optional, if running outside Docker)
- GNU Make (`make`) for helper commands

---

### 2. Clone Repo
```bash
git clone https://github.com/<your-org>/<your-repo>.git
cd <your-repo>
```

---

### 3. Setup Environment
Copy `.env.example` to `.env` and adjust values if needed:

```bash
cp .env.example .env
```

By default:
- **BROKER** â†’ `redpanda:9092`
- **KAFKA_TOPIC_BIKE** â†’ `bike`
- **KAFKA_TOPIC_WEATHER** â†’ `weather`
- **S3/MinIO endpoint** â†’ configured in `conf.py`

---

### 4. Run Services (Kafka + Spark + Producers)
Start everything using Docker Compose:

```bash
make up
```

This will:
- Spin up Redpanda (Kafka replacement)  
- Start Spark container  
- Run producers for bike & weather  

---

### 5. Run Spark Streaming Job
Inside container:

```bash
make spark
```

This runs:
```bash
spark-submit   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.1   app/streaming/stream_to_delta.py
```

---

### 6. Inspect Output
- **Bronze Tables** â€“ `s3a://lakehouse/bronze/{bike,weather}`  
- **Silver Tables** â€“ `s3a://lakehouse/silver/{bike_events,weather_events}`  
- **Gold Tables** â€“ `s3a://lakehouse/gold/{bike_usage_hourly, bike_usage_daily, weather_hourly, weather_daily}`  

You can query them via Spark SQL or open them in a Delta-compatible tool.

---

## ğŸ§‘â€ğŸ’» Development & Code Quality

We enforce consistent Python style via **pyproject.toml**.

### Format Code
```bash
make fmt
```

### Lint Code
```bash
make lint
```

### Run All Checks
```bash
make check
```

This runs:
- **Black** (formatter)  
- **Flake8** (PEP8 compliance)  
- **Pylint** (static analysis)  
- **Isort** (import ordering)  

---

## ğŸ“ Example Queries

Once tables are built, you can query things like:

- ğŸš² *Hourly bike utilization by station:*  
```sql
SELECT station_id, hour(event_time) AS hr, avg(utilization_rate)
FROM gold.bike_usage_hourly
GROUP BY station_id, hr;
```

- ğŸŒ¦ *Correlation between rainfall & bike usage:*  
```sql
SELECT w.precip_mm, b.total_bikes
FROM gold.weather_hourly w
JOIN gold.bike_usage_hourly b
  ON w.hour = b.hour AND w.station_id = b.station_id;
```

---

## ğŸ”® Future Improvements
- Add schema registry validation  
- Deploy to Kubernetes  
- Add Grafana/Prometheus monitoring  
- REST API for querying aggregates  

---

## ğŸ¤ Contributing
PRs welcome! Run `make check` before pushing.

---

## ğŸ“œ License
MIT License
